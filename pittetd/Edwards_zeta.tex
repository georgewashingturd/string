\documentclass[aps,preprint,preprintnumbers,nofootinbib,showpacs,prd]{revtex4-1}
\usepackage{graphicx,color}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath,amssymb}
\usepackage{multirow}
\usepackage{amsthm}%        But you can't use \usewithpatch for several packages as in this line. The search 

\usepackage{cancel}

%%% for SLE
\usepackage{dcolumn}   % needed for some tables
\usepackage{bm}        % for math
\usepackage{amssymb}   % for math
\usepackage{multirow}
%%% for SLE -End

\usepackage{ulem}
\usepackage{cancel}

\usepackage{hyperref}
\usepackage{mathrsfs}
\usepackage[top=1in, bottom=1.25in, left=1.1in, right=1.1in]{geometry}

\usepackage{mathtools} % for \DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

%\usepackage{xeCJK}
%\setCJKmainfont{SimSun}

\newcommand{\msout}[1]{\text{\sout{\ensuremath{#1}}}}


%%%%%% My stuffs - Stef
\newcommand{\lsim}{\mathrel{\mathop{\kern 0pt \rlap
  {\raise.2ex\hbox{$<$}}}
  \lower.9ex\hbox{\kern-.190em $\sim$}}}
\newcommand{\gsim}{\mathrel{\mathop{\kern 0pt \rlap
  {\raise.2ex\hbox{$>$}}}
  \lower.9ex\hbox{\kern-.190em $\sim$}}}

%
% Key
%
\newcommand{\key}[1]{\medskip{\sffamily\bfseries\color{blue}#1}\par\medskip}
%\newcommand{\key}[1]{}
\newcommand{\q}[1] {\medskip{\sffamily\bfseries\color{red}#1}\par\medskip}
\newcommand{\comment}[2]{{\color{red}{{\bf #1:}  #2}}}


\newcommand{\ie}{{\it i.e.} }
\newcommand{\eg}{{\it e.g.} }

%
% Energy scales
%
\newcommand{\ev}{{\,{\rm eV}}}
\newcommand{\kev}{{\,{\rm keV}}}
\newcommand{\mev}{{\,{\rm MeV}}}
\newcommand{\gev}{{\,{\rm GeV}}}
\newcommand{\tev}{{\,{\rm TeV}}}
\newcommand{\fb}{{\,{\rm fb}}}
\newcommand{\ifb}{{\,{\rm fb}^{-1}}}

%
% SUSY notations
%
\newcommand{\neu}{\tilde{\chi}^0}
\newcommand{\neuo}{{\tilde{\chi}^0_1}}
\newcommand{\neut}{{\tilde{\chi}^0_2}}
\newcommand{\cha}{{\tilde{\chi}^\pm}}
\newcommand{\chao}{{\tilde{\chi}^\pm_1}}
\newcommand{\chaop}{{\tilde{\chi}^+_1}}
\newcommand{\chaom}{{\tilde{\chi}^-_1}}
\newcommand{\Wpm}{W^\pm}
\newcommand{\chat}{{\tilde{\chi}^\pm_2}}
\newcommand{\smu}{{\tilde{\mu}}}
\newcommand{\smur}{\tilde{\mu}_R}
\newcommand{\smul}{\tilde{\mu}_L}
\newcommand{\sel}{{\tilde{e}}}
\newcommand{\selr}{\tilde{e}_R}
\newcommand{\sell}{\tilde{e}_L}
\newcommand{\smurl}{\tilde{\mu}_{R,L}}

\newcommand{\casea}{\texttt{IA}}
\newcommand{\caseb}{\texttt{IB}}
\newcommand{\casec}{\texttt{II}}

\newcommand{\caseasix}{\texttt{IA-6}}

%
% Greek
%
\newcommand{\es}{{\epsilon}}
\newcommand{\sg}{{\sigma}}
\newcommand{\dt}{{\delta}}
\newcommand{\kp}{{\kappa}}
\newcommand{\lm}{{\lambda}}
\newcommand{\Lm}{{\Lambda}}
\newcommand{\gm}{{\gamma}}
\newcommand{\mn}{{\mu\nu}}
\newcommand{\Gm}{{\Gamma}}
\newcommand{\tho}{{\theta_1}}
\newcommand{\tht}{{\theta_2}}
\newcommand{\lmo}{{\lambda_1}}
\newcommand{\lmt}{{\lambda_2}}
%
% LaTeX equations
%
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\ba}{\begin{array}}
\newcommand{\ea}{\end{array}}
\newcommand{\bit}{\begin{itemize}}
\newcommand{\eit}{\end{itemize}}

\newcommand{\nbea}{\begin{eqnarray*}}
\newcommand{\neea}{\end{eqnarray*}}
\newcommand{\nbeq}{\begin{equation*}}
\newcommand{\neeq}{\end{equation*}}

\newcommand{\no}{{\nonumber}}
\newcommand{\td}[1]{{\widetilde{#1}}}
\newcommand{\sqt}{{\sqrt{2}}}
%
\newcommand{\me}{{\rlap/\!E}}
\newcommand{\met}{{\rlap/\!E_T}}
\newcommand{\rdmu}{{\partial^\mu}}
\newcommand{\gmm}{{\gamma^\mu}}
\newcommand{\gmb}{{\gamma^\beta}}
\newcommand{\gma}{{\gamma^\alpha}}
\newcommand{\gmn}{{\gamma^\nu}}
\newcommand{\gmf}{{\gamma^5}}
%
% Roman expressions
%
\newcommand{\br}{{\rm Br}}
\newcommand{\sign}{{\rm sign}}
\newcommand{\Lg}{{\mathcal{L}}}
\newcommand{\M}{{\mathcal{M}}}
\newcommand{\tr}{{\rm Tr}}

\newcommand{\msq}{{\overline{|\mathcal{M}|^2}}}

%
% kinematic variables
%
%\newcommand{\mc}{m^{\rm cusp}}
%\newcommand{\mmax}{m^{\rm max}}
%\newcommand{\mmin}{m^{\rm min}}
%\newcommand{\mll}{m_{\ell\ell}}
%\newcommand{\mllc}{m^{\rm cusp}_{\ell\ell}}
%\newcommand{\mllmax}{m^{\rm max}_{\ell\ell}}
%\newcommand{\mllmin}{m^{\rm min}_{\ell\ell}}
%\newcommand{\elmax} {E_\ell^{\rm max}}
%\newcommand{\elmin} {E_\ell^{\rm min}}
\newcommand{\mxx}{m_{\chi\chi}}
\newcommand{\mrec}{m_{\rm rec}}
\newcommand{\mrecmin}{m_{\rm rec}^{\rm min}}
\newcommand{\mrecc}{m_{\rm rec}^{\rm cusp}}
\newcommand{\mrecmax}{m_{\rm rec}^{\rm max}}
%\newcommand{\mpt}{\rlap/p_T}

%%%song
\newcommand{\cosmax}{|\cos\Theta|_{\rm max} }
\newcommand{\maa}{m_{aa}}
\newcommand{\maac}{m^{\rm cusp}_{aa}}
\newcommand{\maamax}{m^{\rm max}_{aa}}
\newcommand{\maamin}{m^{\rm min}_{aa}}
\newcommand{\eamax} {E_a^{\rm max}}
\newcommand{\eamin} {E_a^{\rm min}}
\newcommand{\eaamax} {E_{aa}^{\rm max}}
\newcommand{\eaacusp} {E_{aa}^{\rm cusp}}
\newcommand{\eaamin} {E_{aa}^{\rm min}}
\newcommand{\exxmax} {E_{\neuo \neuo}^{\rm max}}
\newcommand{\exxcusp} {E_{\neuo \neuo}^{\rm cusp}}
\newcommand{\exxmin} {E_{\neuo \neuo}^{\rm min}}
%\newcommand{\mxx}{m_{XX}}
%\newcommand{\mrec}{m_{\rm rec}}
\newcommand{\erec}{E_{\rm rec}}
%\newcommand{\mrecmin}{m_{\rm rec}^{\rm min}}
%\newcommand{\mrecc}{m_{\rm rec}^{\rm cusp}}
%\newcommand{\mrecmax}{m_{\rm rec}^{\rm max}}
%%%song

\newcommand{\mc}{m^{\rm cusp}}
\newcommand{\mmax}{m^{\rm max}}
\newcommand{\mmin}{m^{\rm min}}
\newcommand{\mll}{m_{\mu\mu}}
\newcommand{\mllc}{m^{\rm cusp}_{\mu\mu}}
\newcommand{\mllmax}{m^{\rm max}_{\mu\mu}}
\newcommand{\mllmin}{m^{\rm min}_{\mu\mu}}
\newcommand{\mllcusp}{m^{\rm cusp}_{\mu\mu}}
\newcommand{\elmax} {E_\mu^{\rm max}}
\newcommand{\elmin} {E_\mu^{\rm min}}
\newcommand{\elmaxw} {E_W^{\rm max}}
\newcommand{\elminw} {E_W^{\rm min}}
\newcommand{\R} {{\cal R}}

\newcommand{\ewmax} {E_W^{\rm max}}
\newcommand{\ewmin} {E_W^{\rm min}}
\newcommand{\mwrec}{m_{WW}}
\newcommand{\mwrecmin}{m_{WW}^{\rm min}}
\newcommand{\mwrecc}{m_{WW}^{\rm cusp}}
\newcommand{\mwrecmax}{m_{WW}^{\rm max}}

\newcommand{\mpt}{{\rlap/p}_T}

%%%%%% END My stuffs - Stef

\newcommand{\dunno}{$ {}^{\mbox {--}}\backslash(^{\rm o}{}\underline{\hspace{0.2cm}}{\rm o})/^{\mbox {--}}$}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\DeclareMathOperator{\re}{Re}


\begin{document}

\title{Harold Edwards Riemann's zeta function}
\bigskip
\author{Stefanus Koesno$^1$\\
$^1$ Somewhere in California\\ San Jose, CA 95134 USA\\
}
%
\date{\today}
%
\begin{abstract}

\end{abstract}
%
\maketitle

\renewcommand{\theequation}{A.\arabic{equation}}  % redefine the command that creates the equation no.
\setcounter{equation}{0}  % reset counter 

{\bf Page 4}. Footnote, 
%
\nbea
\int_2^L \frac{1}{\log t}dt \sim \frac{L}{\log L}
\neea
%
We might be able to do this using repeated IBPs, with $u = 1/\log t$ and $dv = dt$
%
\nbea
\int_2^L \frac{1}{\log t}dt & = & \left. \frac{t}{\log t}\right|_{2}^{L} + \int_2^L \frac{1}{\log^2 t}dt \\
& = & \frac{L}{\log L} + C_1 + \int_2^L \frac{1}{\log^2 t}dt
\neea
%
we can either repeat the process or do the following
%
\nbea
\int_2^L \frac{1}{\log^2 t}dt & < & \int_2^{L^{1/2}} \frac{1}{\log^2 2}dt + \int_{L^{1/2}}^L \frac{1}{\log^2 L^{1/2}}dt \\
& < & \frac{L^{1/2} - 2}{\log^2 2}  + \frac{L-L^{1/2}}{\log^2 L^{1/2}} \\
& < & \frac{L^{1/2}}{\log^2 2}  + \frac{L}{\log^2 L^{1/2}}
\neea
%
such that
%
\nbea
\int_2^L \frac{1}{\log t}dt & < & \frac{L}{\log L} + C_1 + C_2 L^{1/2} + \frac{L}{\frac{1}{4}\log^2 L}
\neea
%
if we divide both sides by $L/\log L$ and take the limit $L\to\infty$
%
\nbea
\lim_{L\to\infty} \frac{\int_2^L \frac{1}{\log t}dt}{L/\log L} & < & 1 + 0 + \frac{\log L}{L^{1/2}} + \frac{1}{\frac{1}{4}\log L} \\
& < & 1
\neea
%
recall that $\log x$ is smaller than any $x^{1/k}$. But, on the other hand,
%
\nbea
\int_2^L \frac{1}{\log t}dt & > & \int_2^L \frac{1}{\log L}dt \\
& > & \frac{L-2}{\log L} \\
\lim_{L\to\infty} \frac{\int_2^L \frac{1}{\log t}dt}{L/\log L} & > & 1 
\neea
%
thus
%
\nbea
\int_2^L \frac{1}{\log t}dt & \sim & \frac{L}{\log L}
\neea
%
A small note, this asymptotic equality is not like a normal equality, it is more like a Big Oh, say
%
\nbea
f(L) \sim L + L^{1/2}
\neea
%
we can also say
%
\nbea
f(L) \sim L
\neea
%
because $L^{1/2}/L$ approaches zero when $L\to\infty$, so if we divide both sides by $L$ and then take $L\to\infty$ we get the previous asymptotic expression.

{\bf Page 8}. The Gamma function shown here $\Pi(s)$ is valid for all $s > -1$, how is this so as
%
\nbea
\int_0^\infty e^{-x} x^{s} dx
\neea
%
blows up for $s < 0$ when $x\to0$. The justification is as follows
%
\nbea
e^{-x} x^{s} \le x^s
\neea
%
for all $x > 0$ no matter what $s$ is so the integral is also capped by ($s > -1$)
%
\nbea
\int_0^1 e^{-x}x^s dx \le \int_0^1 x^s dx & = & \left.\frac{1}{s+1} x^{s+1} \right|_0^1 \\
& = & \frac{1}{s+1}
\neea
%
Another way of looking at it is
%
\nbea
\int_0^\infty e^{-x}x^n dx & = & e^{-x}\frac{1}{n+1}x^{n+1} + \int_0^\infty e^{-x}\frac{1}{n+1}x^{n+1} dx
\neea
%
so as long as $n>-1$ we are fine.


{\bf Page 8}. On the definition of factorial, Euler introduced the integral notation
%
\nbea
n! & = & \int_0^\infty e^{-x} x^n dx
\neea
%
the $e^{-x}$ is there to make sure everything is well behaved at $\infty$, the $x^n$ is there to be IBP'd (differentiated) multiple times so that we get the factor $n!$, they also make sure that the boundary terms are zero.

But Euler had other ideas too, instead of assigning $x^n \to u$ and differentiating it every IBP, we can instead integrate it, \ie assign $x^n dx \to dv$ like so (boundary terms are always zero and also changing $n\to s$)
%
\nbea
\int_0^\infty e^{-x} x^s dx & = & \frac{1}{s+1} \int_0^\infty e^{-x} x^{(s+1)} dx
\neea
%
repeating it $N$ times we get
%
\nbea
\int_0^\infty e^{-x} x^s dx & = & \frac{1}{(s+1)(s+2) \dots (s+N)} \int_0^\infty e^{-x} x^{(s+N)} dx
\neea
%
and this way we can immediately see the poles at $s = -1, -2, \dots$. We now expand the integral in the RHS above in an interesting way, please do no do IBP by differentiating $x^{(s+n)}$ that will undo everything we've just done. There is, however, a curious way to expand it which only works when $N\to\infty$, we write
%
\nbea
(s + N)! & = & N! \cdot (1 + N)(2 + N) \cdots (s+N)
\neea
%
since $s$ is just a (fixed) parameter, it's finite, and so as we take $N\to\infty$
%
\nbea
\lim_{N\to\infty} (1 + N)(2 + N) \cdots (s+N) & \sim & (1 + N)(1 + N) \cdots (1+N) = (1+N)^s
\neea
%
and thus
%
\nbea
\lim_{N\to\infty}\int_0^\infty e^{-x} x^{(s+N)} dx & = & N! (1+N)^s
\neea
%
So, Euler's definition of a factorial was
%
\nbea
s! & = & \lim_{N\to\infty} \frac{1\cdot2\cdots N}{(s+1)(s+2) \dots (s+N)} (1+N)^s
\neea
%
although whether he got it from the integral above or not I don't know.

{\bf Page 9}.
%
\nbea
\int_0^\infty e^{-nx} x^{s-1} dx & = & \frac{(s-1)!}{n^s} \\
\to \sum_{n=1}^\infty \int_0^\infty e^{-nx} x^{s-1} dx & = & \sum_{n=1}^\infty \frac{(s-1)!}{n^s} \\
& = & (s-1)!\sum_{n=1}^\infty \frac{1}{n^s}
\neea
%
so as long as $s > 1$ the sum of the integral converges, now we want to see the other way, summing inside the integral. A few points
%
\bit
\item Swapping sum and integral is always OK, as long as the sum is finite, for infinite sums what we are interchanging are actually limit and integral because $\sum^\infty = \lim_{N\to\infty}\sum^N$
%
\item Geometric series formula is only valid if the sum converges, why? because we start with
%
\nbea
A & = & 1 + r + r^2 + \dots \\
rA & = & r + r^2 + r^3 + \dots
\neea
%
we then substract $A - rA = 1\to A = 1/(1-r)$ and so on, the problem is if $A$ is divergent we are then doing $\infty - \infty$ which might not be equal to one, so if we try to do $A = 1 + 2 + 2^2 + \dots$ using the formula we get $-1$ which is not just wrong but also not divergent (although if you read Stopple's book you can actually do this and it's called Abel sum).

So the point is that we can't just apply the formula and then take the limit $r\to a$, we can only do this if $a = 1$.
\eit
%

Swapping integral and sum
%
\nbea
\int_0^\infty \sum_{n=1}^\infty e^{-nx} x^{s-1} dx & = & \int_0^\infty \left(\sum_{n=1}^\infty e^{-nx}\right) x^{s-1} dx \\
& = & \int_0^\infty \left (\frac{1}{1-e^{-x}} - 1 \right ) x^{s-1} dx \\
& = & \int_0^\infty \left (\frac{e^{-x}}{1-e^{-x}}\right ) x^{s-1} dx \\
& = & \int_0^\infty \frac{x^{s-1}}{e^{x} - 1} dx
\neea
%
the only problem we have here is when $e^{-x} \ge 1$ and this only happens when $x = 0$, so we only have one point of contention, literally, by the way, this is similar to the situation for
%
\nbea
\int_0^1 \frac{1}{\sqrt{x}} dx & = & 2
\neea
%
even though it blows up at $x = 0$, we do this by
%
\nbea
\int_0^1 \frac{1}{\sqrt{x}} dx & = & \lim_{a\to0}\int_a^1 \frac{1}{\sqrt{x}} dx
\neea
%
in our case the integral is also OK because $e^x - 1 \ge x$ for $x \ge 0$ and so
%
\nbea
\int_0^1 \frac{x^{s-1}}{e^{x} - 1} dx & \le & \int_0^1 x^{s-2} dx \\
& \le & \left. \frac{1}{s-1} x^{s-1} \right |_0^1 \\
& \le & \frac{1}{s-1}
\neea
%
since $s > 1$. Note that we can't just take the limit ($s > 1$)
%
\nbea
\lim_{x\to0} \frac{x^{s-1}}{e^x-1} & = & \lim_{x\to0} \frac{(s-1)x^{s-2}}{e^x} \\
& = & \infty, ~~~~~~~~~~{\rm  since~} s > 1.
\neea
%
so at zero the integrand still blows up but the integral is OK.

{\bf Page 10}. Why the minus sign? the integrand is $\frac{(-x)^s}{e^x - 1}$, there's a minus sign in front of $x$. This is due to the choice of contour. Note that we are going from $+\infty$ to left all the way to 0 and then to the right all the way to $+\infty$.

If there's no minus sign and this is the cut we choose, then
%
\nbea
x^s & = & e^{s\log x + 0\cdot\pi i} , ~~~~~~~~~~ {\rm going~from~} +\infty {\rm ~to~} 0 \\
x^s & = & e^{s\log x + 2\cdot\pi i} , ~~~~~~~~~~ {\rm going~from~} 0 {\rm ~to~} +\infty
\neea
% 
and when we add the two integrals we get zero
%
\nbea
e^{s\log x + 0\cdot\pi i} - e^{s\log x + 2\cdot\pi i} & = & 0
\neea
%
so if we don't want the minus sign we need to choose the other cut, coming from $-\infty$ to zero and back to $-\infty$ so that the phases will be $\pi i$ and $-\pi i$.

{\bf Page 12}. Order of limit taking around the small semi circle, Bernoulli's number, the integral ($x = \delta e^{i\theta}$)
%
\nbea
\int_0^{2\pi} x^{m-n-1} d\theta & = & \frac{\delta^{m-n-1}}{m-n-1} (e^{(m-n-1) \cdot 2\pi \cdot i} - e^{(m-n-1) \cdot 0 \cdot i})
\neea
%
if $m-n-1 < 0$ then the delta term will blow up, but the $e$ difference is zero no matter what, so this means we need to calculate the $e$ difference first before taking the limit.

{\bf Page 13}. Compact here means closed and bounded.

The whole integral is zero thanks to a corollary of Goursat theorem which says that
%
\nbea
\oint_C f(z) dz & = & \sum_{k=1}^n \oint_{C_k} f(z) dz
\neea
%
where the contours $C_k$'s are all inside of $C$.

{\bf Page 13}. For the integral
%
\nbea
\int_{|y|=\epsilon} (-2\pi i n - y)^{s-1} \frac{y}{e^y - 1} \cdot \frac{dy}{y}
\neea
%
we rewrite as
%
\nbea
\int_{|y|=\epsilon} \frac{f(y)}{y - 0} ~ dy, ~~~~~~~~~~ f(y) = (-2\pi i n - y)^{s-1} \frac{y}{e^y - 1}
\neea
%
we need to remember that $f(y)$ has no pole when $y\to 0$, so the value of the integral is just $2\pi i f(0) = 2\pi i(-2\pi i n)^{s-1}$. Note that
%
\nbea
\lim_{y\to0} \frac{y}{e^y - 1} & = & \lim_{y\to0} \frac{1}{e^y} = 1
\neea
%
after L'Hospital.

{\bf Multiple Pages}. A note on analytic continuation. For gamma function, integral is valid only for $s > 1$ but once we get the final form it applies for all $s$. It is even in the derivation of the analytic continuation
%
\nbea
\int_0^\infty e^{-x} x^{s} dx & = & \int_0^1 e^{-x} x^{s} dx + \int_1^\infty e^{-x} x^{s} dx
\neea
%
for the first integral we expand $e$
%
\nbea
\int_0^1 e^{-x} x^{s} dx & = & \int_0^1 \sum_{n=0}^\infty \frac{(-x)^n}{n!} x^{s} dx \\
& = & \sum_{n=0}^\infty (-1)^{n} \int_0^1  \frac{x^{n+s}}{n!} dx
\neea
%
now this integral is not valid for $s \le -1$, however, a this stage we still assume $s > -1$, once we have the full result
%
\nbea
\int_0^\infty e^{-x} x^{s} dx & = & \sum_{n=1}^\infty (-1)^{n-1} \frac{1}{n!(s+n)} + \int_1^\infty e^{-x} x^{s} dx
\neea
%
we say that this result is valid for all $s$, the derivation is not valid for all $s$ but the final result is, that's waht analytic continuation is.

The same goes with the zeta function, in the derivation on Page 10, we take the small semi circle $\delta \to 0$ only for $s > 1$, once we get the final form we say it applies for all $s$ but of course for $s \le 1$ we cannot take $c\to0$ when evaluating the integral, otherwise the integral around that semi circle blows up and we don't have an entire function anymore.

Another remark is that the radius of that small circle must be $0 < \delta < 2\pi$, because at $2\pi i$ we get an infinity from $1/(e^x-1)$.

{\bf Page 15}.
%
\nbea
\frac{1 + 2\psi(x)}{1 + 2\psi(1/x)} = \frac{1}{\sqrt{x}}
\neea
%
for small $x$
%
\nbea
\psi(1/x) & = & \sum e^{-n^2 \pi 1/x} \\
\lim_{x\to 0} \psi(1/x) & = & \sum e^{-n^2 \pi \infty} \\
& = & 0
\neea
%
so for small $x$ we have
%
\nbea
\lim_{x\to 0} \frac{1 + 2\psi(x)}{1 + 2\psi(1/x)} = 1 + 2\psi(x) & = & \frac{1}{\sqrt{x}} \\
\to \psi(x) & = & \frac{1}{2} \left( \frac{1}{\sqrt{x}} - 1\right)
\neea
%

{\bf Page 24}. Fourier transform, I thought I was an expert LOL well, not compared to Riemann
%
\nbea
\int_0^\infty J(x) x^{-s-1} dx & = & \int_0^\infty J(x) x^{-s} \frac{dx}{x}
\neea
%
every time you see $\frac{dx}{x}$ you should immediately scream $\log x$ because $d(\log x) = \frac{dx}{x}$, let $w = \log x$
%
\nbea
\int_0^\infty J(x) x^{-s} \frac{dx}{x} & = & \int_{-\infty}^\infty J(e^w)e^{-ws} dw \\
& = & \int_{-\infty}^\infty J(e^w)e^{-w(\sigma + it)} dw \\
& = & \int_{-\infty}^\infty \left\lbrack J(e^w)e^{-w\sigma} \right\rbrack e^{-itw} dw
\neea
%

{\bf Page 21}. Something I'm not quite sure, ``to prove the absolute convergence of $\prod(1 + a_i)$ it suffices to prove the absolute convergence of the sum $\sum {a_i}$''. To see this, take the log of the product
%
\nbea
\log \prod(1 + a_i) & = & \sum \log(1 + a_i)
\neea
%
But we know that $\log(1 + x) < x$, so if $\sum a_i$ converges then by comparison test the log of the product as well, but if it converges then it converges to a constant and the infinite product is just the exponentiation of a finite constant and thus converges as well.

{\bf Page 25}. Another useful application of IBP, if the integral doesn't converge (in this case due to oscillations), do IBP :)

{\bf Page 26}. about the ambiguity of $\log(1 - s/\rho)$, especially for $\rho$ which are not large, this is because the convention we use is given in Page 27, \ie $\log(z)$ is defined for all $z$ other than real $z \le 0$ and that for real $z$, $\log(z)$ is real.

So if $\rho$ is smaller than $s$ we can have a situation where $(1 - s/\rho)$ is real and negative, but $\log(s-\rho)$ cannot be real and negative when we set $a > 1$ since we know that the zeros are all in the critical strip (see Section 1.9)  and for $\log(-\rho)$, since $\rho$ is never real we are OK since the argument to the log function is never real and negative.

{\bf Page 27}. On the Fourier inversion of $\frac{1}{s-\beta}$, note that the limit of the integration has to go from $-\infty$ to $\infty$, here the function is defined only from $1$ to $\infty$, what this means is that from $-\infty$ to 1 the value of the function is zero.

{\bf Page 28}. Changing variable of integration from the 0 to 1 plus upper semicircle plus 1 to $x$ into $u = \log t$, we see something quite strange. If we change $[0,1]$ using $u = \log t$ we should get $[-\infty,0]$ but instead we get $[-\infty + i\delta, 0 + i\delta]$, in the book the integration is split into $[i\delta-\infty,i\delta+\log x]$.

This is actually more in line to shifting the whole original contour to be slightly above the positive $x$ axis, rather than doing an upper semicircle. If we push it up the argument of the log has to be positive thus the addition of $i\delta$.

{\bf Page 29}. Here we show that $H(\beta)\to0$ as $\tau\to\infty$, but since $H(\beta)$ is extremely similar to $F(\beta)$, the difference is only in the 
%
\nbea
\log\lbrack 1 - (s/\beta)\rbrack \longleftrightarrow \log\lbrack (s/\beta) - 1 \rbrack 
\neea
%
so the key difference here is that for $H(\beta)$, when we take $\tau\to\infty$, we get $\log\lbrack 1 - (s/\beta)\rbrack \to \log(1)$ while for $F(\beta)$ we get $\log(-1)$. And here we see some application of Lebesgue integral that allows us to interchange limit and integration.

Looking back, this might be the exact reason why we introduce $H(\beta)$ in the first place, we know we need to take $\tau\to\infty$ (recall that $\beta = \sigma + i\tau$), but doing this in $F(\beta)$ causes something like $\log(-1)$, if we reverse the sign inside the log, we might be able to get something, but first we need to see if this log reversed function is the same as our original function, the good news is that in this case they are only separated by a constant $i\pi$.

{\bf Page 30}. Pairing $\rho$ and $1-\rho$, in case you forget, this was discussed back on Page 21.

{\bf Page 31}. $-\zeta(0) = \frac{1}{2}$, here we are using Abel's sum or from the Bernoulli number formula thingy
%
\nbea
\zeta(2n) & = & (-1)^{n+1} \frac{(2\pi)^{2n}}{2} \frac{B_{2n}}{2n!}
\neea
%
and setting $n = 0$, a more interesting way is using Abel's sum but not on $\zeta(0)$ directly because
%
\nbea
\zeta(0) & = & 1 + 1 + 1 + \dots \\
& = & \frac{1}{1-1} \\
& = & \infty
\neea
%
instead we do Abel's summation on 
%
\nbea
\Phi(s) & = & (1 - 2^{1-s})\zeta(s) = 1 - \frac{1}{2^s} + \frac{1}{3^s} - \frac{1}{4^s} + \frac{1}{5^s} \dots = \sum_{n=1}\frac{(-1)^{n-1}}{n^s}
\neea
%
the zero $(1 - 2^{1-s})$ at $s = 1$ cancels the pole of $\zeta(s)$ at $s=1$. Now, for $\Phi(s=0)$ we get
%
\nbea
\Phi(0) & = & 1 - 1 + 1 - 1 + 1 \dots \\
& = & \frac{1}{1 + 1} \\
& = & \frac{1}{2}
\neea
%
instead of $\frac{1}{1-1}$ (again, the pole at $s = 1$ was canceled), thus
%
\nbea
\zeta(0) & = & (1 - 2^{1})^{-1} \Phi(0) \\
& = & (-1) \frac{1}{2}
\neea
%

{\bf Page 42}. The curious thing about the application of Jensen's theorem here is that instead of $\xi(0)$ we get $\xi(1/2)$, the thing to note is that the radius used in Jensen's Theorem is $|z| \le R$, here it is $|s-\frac{1}{2}| \le R$, so the prerequisite is not met :)

We just need to do a simple change of variable $w = s - \frac{1}{2}$ and thus $|w| \le R$
%
\nbea
\xi(s) & = & \xi(w + \frac{1}{2}) \\
\to f(w) & = & \xi(w + \frac{1}{2})
\neea
%
and we do Jensen's Theorem on $f(w)$ instead, in this case
%
\nbea
f(w = 0) & = & \xi\left(\frac{1}{2}\right)
\neea
%
and the zeros of $f(w)$ are now located at $\rho-\frac{1}{2}$ where $\xi(\rho) = 0$, all done :)

{\bf Page 42}. This statement, ``Since all but a finite number of roots $\rho$ satisfy the inequality''
%
\nbea
\frac{1}{|\rho(1-\rho)|} = \frac{1}{|(\rho-\frac{1}{2})^2 - \frac{1}{4}|} < \frac{1}{|\rho-\frac{1}{2}|^2}
\neea
%
let $\rho - \frac{1}{2} = a + ib$ then
%
\nbea
|(\rho-\frac{1}{2})^2 - \frac{1}{4}|^2 & = & (a^2 + b^2 - \frac{1}{4}) + 4a^2b^2 \\
|(\rho-\frac{1}{2})^2|^2 & = & (a^2 + b^2)^2
\neea
%
taking the difference
%
\nbea
|(\rho-\frac{1}{2})^2 - \frac{1}{4}|^2 - |(\rho-\frac{1}{2})^2|^2 & = & \frac{(8a^2 - 1)(8b^2 - 1)}{16}
\neea
%
so for $a^2 > \frac{1}{8}$ and $b^2 > \frac{1}{8}$ we get the inequality in the book, thus only for those roots whose $\Re(\rho-\frac{1}{2})^2 < \frac{1}{8}$ or $\Im(\rho-\frac{1}{2})^2 < \frac{1}{8}$ we don't get the inequality in the book.

{\bf Page 43}. Note that this Theorem and the preceding one all rely on the fact that $R$ is sufficiently large, the inequality
%
\nbea
n(R) \le 3R\log R
\neea
%
doesn't work for all $R$, only for $R$ sufficiently large.

Also this is about seeing how dense the roots of $\xi(s)$ are, note that for example $\sum 1/p$, although is not as dense as $\sum 1/n$, still diverges.

{\bf Page 44}. Dealing with $u_R(s)$, we invoke properties of harmonic functions, first a harmonic function is just a holomorphic function because it is a function that satisfies Cauchy-Riemann conditions, or in other words it satisfies Laplace's equation
%
\nbea
\partial_x^2 f + \partial_y^2 f & = & 0
\neea
%
and because of this very reason a harmonic function cannot have a max or min within a compact domain, because if $x$ is a max/min then it's second derivative in $x$ must be non-zero but due to Laplace's equation above the second derivative in the $y$ has the opposite sign and therefore it is a saddle point.

In this case the domain is not really compact because we exclude the zeros of $\xi$, this is why the book explains that near this point the value is near $-\infty$, so there's no chance to have a max point near these points and the max points should be at the boundary.

{\bf Page 46}. $F(s)$ is an even function of $s - \frac{1}{2}$. We know that $\xi(s)$ is symmetric under $s\to 1-s$, thus
%
\nbea
\frac{\xi(1-s)}{\prod \left \lbrack 1 - \frac{(1-s) - \frac{1}{2}}{\rho - \frac{1}{2}}\right \rbrack} & = & \frac{\xi(s)}{\prod \left \lbrack 1 - \frac{\frac{1}{2}-s}{\rho - \frac{1}{2}}\right \rbrack}
\neea
%
However, if $\rho$ is a zero then $1 - \rho$ is also a zero, thus
%
\nbea
\frac{\xi(s)}{\prod \left \lbrack 1 - \frac{\frac{1}{2}-s}{\rho - \frac{1}{2}}\right \rbrack} & = & \frac{\xi(s)}{\prod \left \lbrack 1 - \frac{\frac{1}{2}-s}{\frac{1}{2} - \rho}\right \rbrack} = \frac{\xi(s)}{\prod \left \lbrack 1 - \frac{-(s - \frac{1}{2})}{-(\rho-\frac{1}{2})}\right \rbrack} = \frac{\xi(s)}{\prod \left \lbrack 1 - \frac{s - \frac{1}{2}}{\rho-\frac{1}{2}}\right \rbrack}
\neea
%
which is all good and all but this is a symmetry under $s\to1-s$, to make it into a symmetry under $s - \frac{1}{2}$ we do the following
%
\nbea
\xi(s) & = & \xi((s - \frac{1}{2}) + \frac{1}{2})
\neea
%
so that under $s - \frac{1}{2} \to \frac{1}{2} - s$ we get
%
\nbea
\xi((s - \frac{1}{2}) + \frac{1}{2}) & \to & \xi((\frac{1}{2} - s) + \frac{1}{2}) \\
& = & \xi(1 - s) \\
& = & \xi(s) \\
& = & \xi((s - \frac{1}{2}) + \frac{1}{2})
\neea
%

{\bf Page 47}. Footnote, first the validity of the product formula for $\sin \pi s$, note that it is easier to show the validity of the product formula for $\frac{\sin \pi s}{\pi s}$, using the Theorem in Section 2.7, we see that $\left (\frac{\sin \pi s}{\pi s}\right )/\left (\prod \left \lbrack 1 - \frac{s^2}{n^2}\right \rbrack \right )$ is an even function and it does grow more slowly than $|s|^2$. Thus
%
\nbea
\frac{\sin \pi s}{\pi s} & = & c \prod \left ( 1 - \frac{s}{n} \right ) \left ( 1 + \frac{s}{n} \right ) \\
& = & c \prod \left ( 1 - \frac{s^2}{n^2} \right )
\neea
%
where we have paired the roots to ensure the infinite product is convergent. Next
%
\nbea
\frac{(\sin \pi s)/(\pi s)}{(\sin \pi 0)/(\pi 0)} & = & \frac{\prod \left ( 1 - \frac{s^2}{n^2} \right )}{1}
\neea
%
But $(\sin \pi 0)/(\pi 0) = 1$ thus
%
\nbea
\frac{\sin \pi s}{\pi s} & = & \prod \left ( 1 - \frac{s^2}{n^2} \right )
\neea
%
The second footnote is not related to this one, it just says that everything in that page has been rigorously proven, including the product formula for sin, the only thing not proven is the relationship between (2) and (3).

{\bf Page 50}. Footnote, logarithmic differentiation of $\zeta(s)$
%
\nbea
\frac{d(\log \zeta(s))}{ds} = \frac{\zeta'(s)}{\zeta(s)} & = & \frac{d}{ds} \left ( \log\prod (1 - p^{-s})^{-1}\right ) \\
& = & -\sum_p \frac{d}{ds} \log(1 - p^{-s}) \\
& = & -\sum_p \frac{\log p}{1 - p^{-s}}
\neea
%
then
%
\nbea
\frac{1}{1 - p^{-s}} & = & 1 + p^{-s} + p^{-2s} + \dots
\neea
%
so
%
\nbea
-\sum \frac{\log p}{1 - p^{-s}} & = & -\sum_p \log p \sum_k p^{-ks}
\neea
%
and we are done :)

{\bf Page 51}. formula (2) of Section 1.14 is
%
\nbea
\frac{1}{2\pi i} \int_{a - i\infty}^{a + i\infty} \frac{1}{s - \beta} y^s ds & = & \left \{
\begin{array}{ll}
y^\beta &~~~~~ y > 1 \\
0 &~~~~~ y < 1
\end{array}
\right.
\neea
%



{\bf Page 52}. Bottom page, about the middle equation, say $\beta = A + iB$, here we don't take the limit $h\to\infty$ yet
%
\nbea
\int_{a - \beta - ih}^{a - \beta + ih} & = & \int_{a - A - iB - ih}^{a - A - iB + ih} \\
& = & \int_{a - A - iB - ih}^{a - A - ih} + \int_{a - A - ih}^{a - A + ih} + \int_{a - A + ih}^{a - A -iB + ih} \\
& = & \int_{a - A - iB - ih}^{a - A - ih} + \int_{\Re{(a - \beta)} - ih}^{\Re{(a - \beta)} + ih} + \int_{a - A + ih}^{a - A -iB + ih} 
\neea
%
so now we have to make sure that these side band integrals are negligible, let $t = a - A + i\tau$
%
\nbea
\int_{a - A + ih}^{a - A -iB + ih}  \frac{x^t}{t} dt & = & i\int_{h}^{(h - B)} \frac{e^{(a-A+i\tau)\log x}}{a -  A + i\tau} d\tau \\
& \le & \int_{h}^{(h - B)} \frac{|e^{(a-A+i\tau)\log x}|}{|a -  A + i\tau|} d\tau \\
& \le & e^{a - A} \int_{h}^{(h - B)} \frac{1}{|i\tau|} d\tau \\
& \le & e^{a - A} \log (1 - \frac{B}{h})
\neea
%
if we take the limit $h\to\infty$ we get $\to\log 1$ which approaches zero :) we can do the same with the lower side band and get the same result. The lesson here is that we don't need exact things, \ie we don't need it to be exactly zero, approaching zero is good enough and this I see over and over in analytic number theory.

{\bf Multiple Pages}. I find it interesting that to prove that the limit of the sum is the sum of the limit we just need to show that the latter converges but we did not need to show that they converge to the same value.

{\bf Page 55}. The integrand in the second integral is an odd function
%
\nbea
i\int_{-h}^{h} \frac{t}{a^2 + t^2} dt
\neea
%
and it's integrated in a symmetric domain so the integral is zero :)

{\bf Page 55}. This is the normal trig trick :)
%
\nbea
\int \frac{1}{1+u^2} du & = & \int \frac{1}{1 + \tan^2 w} \frac{dw}{\sec^2 w}, ~~~~~ \frac{1}{1 + \tan^2 w} = \sec^2 w \\
& = & \int dw \\
& = & w = \arctan u
\neea
%

{\bf Page 56}. Inequality on first line
%
\nbea
(a - c)^2 = a^2 - 2ac + c^2 & > & 0 \\
a^2 + c^2 & > & 2ac \\
2(a^2 + c^2) & > & a^2 + 2ac + c^2 \\
a^2 + c^2 & > & \frac{(a+c)^2}{2} \\
\sqrt{a^2 + c^2} & > & \frac{(a+c)}{\sqrt{2}} \\
\to |a + ic| & > & \frac{(a+c)}{\sqrt{2}}
\neea
%

{\bf Page 58}. And again, just for fun
%
\nbea
\cos \frac{\pi}{4} = \cos 2 \frac{\pi}{8} & = & 2 \cos^2 \frac{\pi}{8} - 1 \\
\frac{1}{\sqrt{2}} & = & 2 \cos^2 \frac{\pi}{8} - 1 \\
\to \cos \frac{\pi}{8} & = & \left(\frac{1 + \frac{1}{\sqrt{2}}}{2}\right)^{1/2}
\neea
%
by the same token
%
\nbea
\cos \frac{\pi}{4} = \cos 2 \frac{\pi}{8} & = & 1 - 2 \sin^2 \frac{\pi}{8} \\
\to \sin \frac{\pi}{8} & = & \left(\frac{1 - \frac{1}{\sqrt{2}}}{2}\right)^{1/2}
\neea
%
and so
%
\nbea
\tan \frac{\pi}{8} & = & \left (\frac{\frac{1 - \frac{1}{\sqrt{2}}}{2}}{\frac{1 + \frac{1}{\sqrt{2}}}{2}} \right )^{1/2} = \left (\frac{1 - \frac{1}{\sqrt{2}}}{1 + \frac{1}{\sqrt{2}}} \right )^{1/2} = \left (\frac{1 - \frac{1}{\sqrt{2}}}{1 + \frac{1}{\sqrt{2}}} \times \frac{1 + \frac{1}{\sqrt{2}}}{1 + \frac{1}{\sqrt{2}}}\right )^{1/2} \\
& = & \left(\frac{1 - \frac{1}{2}}{\left(1 + \frac{1}{\sqrt{2}}\right)^2} \right) ^{1/2} = \frac{1}{1 + \frac{1}{\sqrt{2}}}\cdot \frac{1}{\sqrt{2}} = \frac{1}{1 + \sqrt{2}} < \frac{1}{2}
\neea
%

{\bf Page 65}. Evaluating integrals
%
\nbea
\int_0^K \frac{1}{r-1} dr
\neea
%
there's a pole here at $r = 1$, this is almost the same as integrating $dz/z$, so let's make the change of variable $z = r-1, dz = dr$
%
\nbea
\int_0^K \frac{1}{r-1} dr = \int_{-1}^{K-1} \frac{1}{z} dz & = & \int_{-1}^{0-\delta} \frac{1}{z} dz  + \int_C \frac{1}{z} dz + \int_{0+\delta}^{K-1} \frac{1}{z} dz
\neea
%
where the contour $C$ for the middle integral is an upper semicircle above $z = 0$, 
%
\nbea
\int_C \frac{1}{z} dz & = & \int_{\pi}^{0} \frac{1}{\delta e^{i\theta}} d(\delta e^{i\theta}) \\
& = & i\int_{\pi}^{0} \frac{\delta e^{i\theta}}{\delta e^{i\theta}} d\theta \\
& = & -i\pi
\neea
%
Now for the other integrals we do the following
%
\nbea
\int_{-1}^{0-\delta} \frac{1}{z} dz  + \int_{0+\delta}^{K-1} \frac{1}{z} dz & = & \int_{1}^{0+\delta} \frac{1}{-z} d(-z)  + \int_{0+\delta}^{1} \frac{1}{z} dz + \int_{1}^{K-1} \frac{1}{z} dz \\
& = & -\int_{0+\delta}^{1} \frac{1}{z} dz  + \int_{0+\delta}^{1} \frac{1}{z} dz + \int_{1}^{K-1} \frac{1}{z} dz \\
& = & \int_{1}^{K-1} \frac{1}{z} dz
\neea
%
so we first do $z\to-z$ on the first integral and then swap the integration limits, for the second integral we split the integral, this way we can cancel the blowing up parts and are left with $\log(K-1)$.

We can also shift the pole by the usual trick
%
\nbea
\int_{-1}^{K-1} \frac{1}{z} dz \to \int_{-1}^{K-1} \frac{1}{z + i\epsilon} dz & = & \log (K - 1 + i\epsilon) - \log(-1 + i\epsilon)
\neea
%
taking the limit $\epsilon\to0$ is a bit tricky because the log is a bit ambiguous, is $\log (-1) = i \pi$ or $-i\pi$?

{\bf Page 69}. These equations are quite easily derived using von Mangoldt's method :) For me the starting point is Section 3.7, Eq. (1) - (3) which culminates in the following equation 
%
\nbea
\int_0^x t^{-r} d\psi(t) & = & \frac{x^{1-r}}{1-r} - \sum_\rho \frac{x^{\rho-r}}{\rho-r} + \sum_n \frac{x^{-2n-r}}{2n+r} - \frac{\zeta'(r)}{\zeta(r)}
\neea
%
and for what we need
%
\nbea
\int_0^x \left(\frac{x}{t}\right)^{r} d\psi(t) & = & \frac{x}{1-r} - \sum_\rho \frac{x^{\rho}}{\rho-r} + \sum_n \frac{x^{-2n}}{2n+r} - x^r\frac{\zeta'(r)}{\zeta(r)}
\neea
%


If we set $r=0$ we get
%
\nbea
\int_0^x t^{-0} d\psi(t) = \int_0^x t^{-0} d\psi(t) = \int_0^x d\psi(t) = \psi(x) & = & x - \sum_\rho \frac{x^{\rho}}{\rho} + \sum_n \frac{x^{-2n}}{2n} - \frac{\zeta'(0)}{\zeta(0)}
\neea
%
although this still needs all those arguments about swapping sums, integrals and limits :)

To get de la Vallee Poussin's Eq (3) we need the following IBP 
%
\nbea
\int_0^x \log \frac{x}{t} d\psi(t) & = & \left. \log \frac{x}{t} \psi(t) \right|_0^x + \int_0^x t^{-1} \psi(t) dt \\
& = & \psi(x) \log \frac{x}{x} - \psi(0)\log \frac{x}{0} + \int_0^x t^{-1} \psi(t) dt
\neea
%
Note that $\psi(0) = 0$, there's no limit taken here so
%
\nbea
\psi(0)\log \frac{x}{0} & = & \lim_{r\to0} 0 \cdot \log \frac{x}{r}
\neea
%
and therefore it is zero and from the main equation of Section 3.7 as stated above we do the following (differentiating inside the integral)
%
\nbea
\frac{d}{dr}\left.\left(\int_0^x \left(\frac{x}{t}\right)^{r} d\psi(t)\right)\right|_{r = 0} & = & \int_0^x (\log \frac{x}{t}) d\psi(t)\\
\int_0^{x} t^{-1} \psi(t) dt & = & x - \sum_\rho \frac{x^\rho}{\rho^2} - \sum_n \frac{x^{-2n}}{(2n)^2} - \log x \frac{\zeta'(0)}{\zeta(0)}  - \frac{\zeta''(0)\zeta(0) - \zeta'^2(0)}{\zeta^2(0)} \\
\to \int_0^{x} t^{-1} \psi(t) dt & = & x -\sum_\rho \frac{x^\rho}{\rho} - \sum_n \frac{x^{-2n}}{(2n)^2} - \log x\frac{\zeta'(0)}{\zeta(0)} + {\rm const.}
\neea
%
This way we don't need the explicit formula for $\psi(x)$.

For Hadamard's Eq (2) what we need is the footnote on Page 74. First we reuse the (modified) main formula from Page 69, shown above, \ie the formula for $\int_0^x \left(\frac{x}{t}\right)^{r} d\psi(t)$, once we have this, we do the following
%
\nbea
\int_0^x \left(\frac{x}{t}\right)^{r} d\psi(t) - \int_0^x \left(\frac{x}{t}\right)^{w} d\psi(t) & = & \left(\frac{x}{1-r} - \frac{x}{1-w} \right ) - \sum_\rho \left(\frac{x^{\rho}}{\rho-r} - \frac{x^{\rho}}{\rho-w} \right) + \\
&& \sum_n \left(\frac{x^{-2n}}{2n+r} - \frac{x^{-2n}}{2n+w}\right) - \left(x^r\frac{\zeta'(r)}{\zeta(r)} - x^w\frac{\zeta'(w)}{\zeta(w)} \right ) \\
& = & x \frac{(r-w)}{(r-1)(w-1)} - \sum_\rho \frac{x^{\rho} (r-w)}{(\rho-r)(\rho-w)} \\
&& - \sum_n \frac{x^{-2n}(r-w)}{(2n+r)(2n+w)} - \left(x^r\frac{\zeta'(r)}{\zeta(r)} - x^w\frac{\zeta'(w)}{\zeta(w)} \right )
\neea
%
which is the formula given in the footnote, and heeding the footnote :) to get (2) we set $r = 1, w = 0$ and do IBP on the Left Hand Side
%
\nbea
\int_0^x \frac{x}{t} d\psi(t) - \int_0^x d\psi(t) & = &  \left.\frac{x}{t} \psi(t) \right |_0^x + \int_0^x \frac{x}{t^2} \psi(t) dt - \psi(x) \\
& = & \left ( \bcancel{\psi(x)} - \frac{x\psi(x)}{0}\right ) + x \int_0^x t^{-2} \psi(t) dt - \bcancel{\psi(x)} \\
& = & x \int_0^x t^{-2} \psi(t) dt
\neea
%
again $\frac{\psi(0)}{0}$ is zero because $\psi(0) = 0$. The RHS is straightforward, except for the first term, therefore we have to combine this with the last term because these infinities cancel out
%
\nbea
& = & \left (\lim_{r\to1} x \frac{1}{1-r} - x^r\frac{\zeta'(r)}{\zeta(r)} \right) - \sum_\rho \frac{x^{\rho} (1-0)}{(\rho-1)(\rho-0)} - \sum_n \frac{x^{-2n}(1-0)}{(2n+1)(2n+0)} + x^0\frac{\zeta'(0)}{\zeta(0)} \\
& = & \left (\lim_{r\to1} x \frac{1}{1-r} - x^r\frac{\zeta'(r)}{\zeta(r)} \right) - \sum_\rho \frac{x^{\rho}}{(\rho-1)\rho} - \sum_n \frac{x^{-2n}}{(2n+1)2n} + \frac{\zeta'(0)}{\zeta(0)}
\neea
%
the limit is calculated by Laurent expanding $\frac{\zeta'(r)}{\zeta(r)}$ and isolate the pole term, however, we incur an extra minus sign (and it has to for it to work, see Lemma 4 Page 289 of Apostol)
%
\nbea
\lim_{r\to 1}\frac{x}{1-r} - x^r\frac{\zeta'(r)}{\zeta(r)} = \lim_{r\to 1}\frac{x}{1-r} - \frac{x^r}{1-r}  + x^r \cdot f(r) & = & \lim_{r\to1} \frac{x-x^{r}}{1-r} + x^r \cdot f(r) \\
& = & \lim_{r\to1} \frac{-x^r \log x}{-1} + x^r \cdot {\rm const.} \\
& = & x \log x + x \cdot {\rm const.}
\neea
%
Combining everything
%
\nbea
x \int_0^x t^{-2} \psi(t) dt & = & x \log x - \sum_\rho \frac{x^{\rho}}{(\rho-1)\rho} - \sum_n \frac{x^{-2n}}{(2n+1)2n} + \frac{\zeta'(0)}{\zeta(0)} + x \cdot {\rm const.} \\
\to \int_0^x t^{-2} \psi(t) dt & = & \log x - \sum_\rho \frac{x^{\rho-1}}{(\rho-1)\rho} - \sum_n \frac{x^{-2n-1}}{(2n+1)2n} + \frac{1}{x}\frac{\zeta'(0)}{\zeta(0)} + {\rm const.}
\neea
%


{\bf Page 70}. The sum
%
\nbea
B = \frac{1}{2} \sum\frac{1}{p^2} + \frac{1}{3} \sum\frac{1}{p^3} + \frac{1}{4} \sum\frac{1}{p^4} + \dots
\neea
%
converges, this is how, for $k \ge 2$
%
\nbea
\sum \frac{1}{p^k} & < & \sum_{n=2} \frac{1}{n^k} < 1
\neea
%
the last inequality can be easily shown using the integral test which shows that
%
\nbea
\sum_{n=2} \frac{1}{n^k} < \int_1^\infty x^{-k} dx & = & \frac{1}{k - 1}
\neea
%
%
\nbea
B = \frac{1}{2} \sum\frac{1}{p^2} + \frac{1}{3} \sum\frac{1}{p^3} + \frac{1}{4} \sum\frac{1}{p^4} + \dots & < & \frac{1}{2} \sum\frac{1}{n^2} + \frac{1}{3} \sum\frac{1}{n^3} + \frac{1}{4} \sum\frac{1}{n^4} + \dots \\
& < & \frac{1}{2} \cdot1 + \frac{1}{3} \cdot\frac{1}{2} + \frac{1}{4} \cdot \frac{1}{3} + \dots \\
& = & \sum_{n=2}\frac{1}{n(n-1)}
\neea
%
and using the integral test this one is also convergent :)

Here's a way that doesn't work, initially I wanted to show that we can massage $B$ into $\sum \frac{1}{p^s}$ with $s > 1$, this is because each $\sum \frac{1}{p^k}$ is less than one, so there must be an $s > 1$ such that 
%
\nbea
\frac{1}{n}\sum\frac{1}{p^n} & = & \frac{1}{p^s}
\neea
%
however, this $s$ is getting smaller and smaller as $p$ gets bigger and it approaches 1, so if we use the same $s$ for all $p$ we get $\sum \frac{1}{p}$ and it doesn't work.





{\bf Page 73}. One trick to calculate $\int_0^x t^{k-1} \psi(t) dt$ is, one, the partial fraction formula
%
\nbea
\frac{1}{s(s+k)} = \frac{1/k}{s} - \frac{1/k}{s + k}
\neea
%
and from this we can see that to get other powers of $x$ we do the following
%
\nbea
\frac{1}{2\pi i} \int_{a - i\infty}^{a + i\infty} \frac{x^{s+1}}{n^s s (s+1)} ds \to \frac{1}{2\pi i} \int_{a - i\infty}^{a + i\infty} \frac{x^{s+k}}{n^s s (s+k)} ds
\neea
%
splitting through partial fraction
%
\nbea
\frac{x^k}{k 2\pi i} \int_{a - i\infty}^{a + i\infty} \left(\frac{x}{n}\right)^{s} \frac {ds}{s}-\frac{n^k}{k 2\pi i} \int_{a - i\infty}^{a + i\infty} \left(\frac{x}{n}\right)^{s+k} \frac {ds}{s+k}
\neea
%
which becomes
%
\nbea
 = \left\{
 \begin{array}{ ll }
 \frac{1}{k} \left ( x^k - n^k\right ) ~~~~~ & {\rm if~} n \le x \\
 0 ~~~~~ & {\rm if~} n \ge x
 \end{array}
 \right.
\neea
%
so what we have is 
%
\nbea
\frac{1}{k} \sum_{n \le x} \Lambda(n) (x^k - n^k) = \frac{1}{k} \int_0^x (x^k - t^k) d\psi(t) = \int_0^x t^{k-1} \psi(t) dt
\neea
%
the last step uses IBP which nicely cancels the minus sign and the factor $\frac{1}{k}$. But of course this method doesn't work for $k = 0.$


{\bf Page 74}. Another trick to calculate $\int_0^x t^{-k} d\psi(t)$ is that the same thing that was done in Chapter 3. The main thing is
%
\nbea
\frac{1}{2\pi i} \int_{a-i\infty}^{a+i\infty} \left\lbrack -\frac{\zeta'(s)}{\zeta(s)} \right \rbrack x^s \frac{ds}{s} & = & \sum_n \Lambda(n) \left \{\frac{1}{2\pi i} \int_{a-i\infty}^{a+i\infty} \left (\frac{x}{n}\right)^s \frac{ds}{s} \right \}
\neea
%
and whatever is inside the curly braces is 1 or 0, which translates to the integral $\int_0^x d\psi(t)$. Now what we want is $\int_0^x \left(\frac{x}{t} \right)^{u} d\psi(t)$ which is equivalent to $\sum_{n \le x} \Lambda(n) \left(\frac{x}{n}\right)^u$ . So instead of 1 or 0, you want the whole curly braces to be $\left(\frac{x}{n}\right)^u$ or 0.

This is definitely easier than the one depicted on Page 73 with partial fraction, we only need to multiply the curly braces with $\left(\frac{x}{n}\right)^u$ 
%
\nbea
\sum_n \Lambda(n) \left \{ \left(\frac{x}{n}\right)^u \frac{1}{2\pi i} \int_{a-i\infty}^{a+i\infty} \left (\frac{x}{n}\right)^s \frac{ds}{s} \right \} & = & \frac{1}{2\pi i} \int_{a-i\infty}^{a+i\infty} \left\lbrack \sum_n \Lambda(n) n^{-s-u}\right \rbrack x^{s+u} \frac{ds}{s} \\
& = & \frac{x^u}{2\pi i} \int_{a-i\infty}^{a+i\infty} \left\lbrack -\frac{\zeta'(s+u)}{\zeta(s + u)} \right \rbrack x^{s} \frac{ds}{s}
\neea
%
but this is exactly what was done in Section 3.7. Note that we can't pull the factor $\left(\frac{x}{n}\right)^u$ out of the sum because it involves $n$ which is summed. So the derivation of this footnote will just utilize the method of Section 3.7, for the full derivation see comments on Page 69.

{\bf Page 82}. How do we compare $x^{-\epsilon}$ and $\exp[-c(\log x)^{1/2}]$, the book claims that $x^{-\epsilon}$ decays much faster, to see this set $y = \log x$ thus
%
\nbea
x^{-\epsilon} \to e^{-\epsilon y} ~~~~~~~{\rm and} ~~~~~~~ \exp[-c(\log x)^{1/2}] \to e^{-c\sqrt{y}}
\neea
%
so for a sufficiently large $y$, $e^{-\epsilon y}$ decays faster. Now how about the absorption into a constant $c$? Since
%
\nbea
e^{-\epsilon y} & = & O(e^{-c\sqrt{y}})
\neea
%
This means that there is a constant $K$ such that for $y > y_0$
%
\nbea
|e^{-\epsilon y}| & < & K|e^{-c\sqrt{y}}|
\neea
%
and the usual trick
%
\nbea
K e^{-c\sqrt{y}} & = & e^{\log K} e^{-c\sqrt{y}} \\
& = & e^{-c\sqrt{y} + \log K} = e^{-c\sqrt{y}(1 - \log K/c\sqrt{y})}
\neea
%
Now
%
\nbea
e^{-c\sqrt{y}(1 - \log K/c\sqrt{y})} > e^{-c\sqrt{y} }
\neea
%
but $\log K/c\sqrt{y}$ gets smaller and smaller as $y$ goes up, so we can get the max of this value, say $K'$ and the whole thing becomes
%
\nbea
e^{-c\sqrt{y}(1 - K')} > e^{-c\sqrt{y}(1 - \log K/c\sqrt{y})} > e^{-c\sqrt{y} + \log K} > e^{-c\sqrt{y}}
\neea
%
thus we can choose a new $c' = c(1-K')$ such that
%
\nbea
e^{-c'\sqrt{y}} > e^{-c\sqrt{y}}
\neea
%

{\bf Page 83}.
%
\nbea
\frac{\psi(x) - x}{x} & \le & K_1 e^{-c_1 (\log x)^{1/2}} \le e^{-c'_1 (\log x)^{1/2}}
\neea
%
and
%
\nbea
\frac{\psi(y) - y}{y} & \ge & -K_2 e^{-c_2 (\log y)^{1/2}} \ge - e^{-c'_2 (\log y)^{1/2}}
\neea
%
combining
%
\nbea
-e^{-c'_2 (\log x)^{1/2}} \le \frac{\psi(x) - x}{x} & \le & e^{-c'_1 (\log x)^{1/2}}
\neea
%
now we can choose  $c'' = \min\{c'_1,c'_2\}$ such that we get a bigger magnitude for the error
%
\nbea
|e^{-c'' (\log x)^{1/2}}| \ge \max \left \{ |e^{-c'_2 (\log x)^{1/2}}|, |e^{-c'_1 (\log x)^{1/2}}|\right \}
\neea
%
Thus the magnitude of the relative error is 
%
\nbea
\left | \frac{\psi(x) - x}{x} \right | & \le & e^{-c'' (\log x)^{1/2}}
\neea
%

{\bf Page 83}. The term relative error here means $\frac{\theta(x) - x}{x}$, so it is divided by $x$, thus it should be that 
%
\nbea
\theta(x) - x \sim x \cdot e^{-c (\log x)^{1/2}}
\neea
%
to show this, we know from its definition
%
\nbea
\psi(x) & = & \theta(x) + \theta(x^{1/2}) + \theta(x^{1/3}) + \dots
\neea
%
so the difference between
%
\nbea
\psi(x) - \theta(x)  & \le & \theta(x^{1/2}) \frac{\log x}{\log 2}
\neea
%
but the difference between
%
\nbea
\psi(x) - x & \le & x e^{-c(\log x)^{1/2}}
\neea
%
but, as stated, $\theta(x^{1/2}) \log x \sim x^{1/2} \log x$ and this is much smaller than $x e^{-c(\log x)^{1/2}}$ (because $\frac{x^{1/2} \log x}{x}$ already goes to zero as $x\to\infty$, thus $\theta(x^{1/2}) \log x = O(x e^{-c(\log x)^{1/2}})$), so the difference between $\theta(x)$ and $x$ is potentially
%
\nbea
\left | \frac{\theta(x) - x}{x}\right | & \le & \frac{\theta(x^{1/2})\log x}{x\log 2} + e^{-c(\log x)^{1/2}} \sim \frac{\log x}{x^{1/2}\log 2} + e^{-c(\log x)^{1/2}} \\
& \le & O(e^{-c(\log x)^{1/2}}) + e^{-c(\log x)^{1/2}} \\
& \le & e^{-c'(\log x)^{1/2}}
\neea
%






































\end{document}